\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Questions}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces For the case of $k=1$, two examples are correctly classified in all the four cases. Thus, $VC(1)=2$.}}{2}}
\newlabel{fig:vc_dimension1}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces When adding $k$-th interval, two additional examples are correctly classified in all the four cases.}}{2}}
\newlabel{fig:vc_dimension2}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This figure shows a case in which $k+1$ positive examples and $k$ negative examples cannot be correctly classified by $k$ disjoint intervals. The right most positive example (red color) is classified incorrectly. Thus, $VC(k) < 2k+1$.}}{3}}
\newlabel{fig:vc_dimension3}{{3}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient descent algorithm applied to hige loss with l2 regularization}}{3}}
\newlabel{euclid}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Programming Assignment}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Gradient descent algorithm applied to hige loss with l1 and l2 regularization}}{5}}
\newlabel{euclid}{{2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces This figure shows the descrese of the objective over the steps when $stepSize=0.0001$ and $labmda=0.01$.}}{6}}
\newlabel{fig:gradient_descent}{{4}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces This figure shows the descrese of the objective over the steps when $stepSize=0.1$ and $labmda=0.1$. As $stepSize$ is relatively large, some oscillation occured.}}{6}}
\newlabel{fig:gradient_descent_oscillation}{{5}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This figure shows the accuracy over the validation data using different pairs of $stepSize$ and $lambda$ when $regularization=l1$ and $featureSet=1$. The performance drastically decreases when $lambda > 1$.}}{7}}
\newlabel{fig:performance_hyperparameter}{{6}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The best combination of $stepSize$ and $lambda$}}{8}}
\newlabel{tab:best_hyperparameters}{{1}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The results of featurSet=1 with $l1$ regularization}}{8}}
\newlabel{tab:result_1_l1}{{2}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The results of featurSet=1 with $l2$ regularization}}{8}}
\newlabel{tab:result_1_l2}{{3}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The results of featurSet=2 with $l1$ regularization}}{9}}
\newlabel{tab:result_2_l1}{{4}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The results of featurSet=2 with $l2$ regularization}}{9}}
\newlabel{tab:result_2_l2}{{5}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces The results of featurSet=3 with $l1$ regularization}}{9}}
\newlabel{tab:result_3_l1}{{6}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces The results of featurSet=3 with $l1$ regularization}}{9}}
\newlabel{tab:result_3_l2}{{7}{9}}
