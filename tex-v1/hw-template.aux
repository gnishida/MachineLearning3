\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Questions}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces For the case of $k=1$, two examples are correctly classified in all the four cases. Thus, $VC(1)=2$.}}{2}}
\newlabel{fig:vc_dimension1}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces When adding $k$-th interval, two additional examples are correctly classified in all the four cases.}}{2}}
\newlabel{fig:vc_dimension2}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This figure shows a case in which $k+1$ positive examples and $k$ negative examples cannot be correctly classified by $k$ disjoint intervals. The right most positive example (red color) is classified incorrectly. Thus, $VC(k) < 2k+1$.}}{3}}
\newlabel{fig:vc_dimension3}{{3}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient descent algorithm applied to hige loss with l2 regularization}}{3}}
\newlabel{euclid}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Programming Assignment}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Gradient descent algorithm applied to hige loss with l1 and l2 regularization}}{4}}
\newlabel{euclid}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces This figure shows the descrese of the objective over the iterations when $featureSet=1$, $stepSize=0.001$, and $labmda=0.4$.}}{5}}
\newlabel{fig:gradient_descent}{{4}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces This figure shows the decrese in the objective over the iterations when $featureSet=2$, $stepSize=0.001$, and $lambda=1.0$.}}{6}}
\newlabel{fig:gradient_descent_large_lambda_1}{{5}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The best combination of $stepSize$ and $lambda$}}{6}}
\newlabel{tab:best_hyperparameters}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This figure shows the accuracy over the validation data using different pairs of $stepSize$ and $lambda$ when $regularization=l2$ and $featureSet=1$. The performance drastically decreases when $lambda > 1$.}}{7}}
\newlabel{fig:performance_hyperparameter}{{6}{7}}
\citation{gao2007}
\citation{tsuruoka2009}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The results of featurSet=1 with $l1$ regularization}}{8}}
\newlabel{tab:result_1_l1}{{2}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The results of featurSet=1 with $l2$ regularization}}{8}}
\newlabel{tab:result_1_l2}{{3}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The results of featurSet=2 with $l1$ regularization}}{8}}
\newlabel{tab:result_2_l1}{{4}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The results of featurSet=2 with $l2$ regularization}}{8}}
\newlabel{tab:result_2_l2}{{5}{8}}
\bibcite{gao2007}{1}
\bibcite{tsuruoka2009}{2}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces The results of featurSet=3 with $l1$ regularization}}{9}}
\newlabel{tab:result_3_l1}{{6}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces The results of featurSet=3 with $l2$ regularization}}{9}}
\newlabel{tab:result_3_l2}{{7}{9}}
